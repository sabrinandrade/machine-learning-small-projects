{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer\n",
    "\n",
    "* Data: Amazon reviews\n",
    "* But the problem has been simplified, instead of 5 stars, the output is classified only as positive or negative\n",
    "* The only input is the \"review_text\", all other data is ignored\n",
    "* BeautifulSoup is a XML Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    sentence = sentence.lower() # All sentences are now in lower case\n",
    "    tokens = nltk.tokenize.word_tokenize(sentence) # Creates tokens\n",
    "    tokens = [t for t in tokens if len(t) > 2] # Ignore all tokens with only one letter\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # Applies the lemmatizer\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english') and t != ''] # Removes stop words and empty tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_reviews = BeautifulSoup(open('sorted_data_acl//electronics//positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text') # Uses only the review_text info\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('sorted_data_acl//electronics//negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text') # Uses only the review_text info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since we have more positive reviews than negatives, we trim the amount of positive reviews\n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary containing every word in the vocabulary\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates the dictionary: defines the vocabulary size and the index of each word\n",
    "# The vocabulary could still be reduced\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "print(word_index_map)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens, label):\n",
    "    # Feature vector has the same size of the vocabulary (+1 for the label) and has all values set to 0\n",
    "    x = np.zeros(len(word_index_map) + 1) \n",
    "    # Find the word in the dictionary, retrieve its index \n",
    "    # Change the value of the word for this particular X vector\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    # Normalizes X in relation to the total amount of words in the document\n",
    "    x = x / x.sum()\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "                \n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "print(\"Rows (input size): \", len(data))\n",
    "print(\"Columns (feature vector size): \", len(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "# You can manually split or use sklearn function\n",
    "\n",
    "#X = data[:, :-1]\n",
    "#Y = data[:, -1:]\n",
    "\n",
    "#X_train = X[:-100,]\n",
    "#Y_train = Y[:-100,]\n",
    "\n",
    "#X_test = X[-100:,]\n",
    "#Y_test = Y[-100:,]\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[:, :-1], \n",
    "                                                    data[:, -1:], \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = 42, \n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "print(\"Classification rate: \", model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLP with two layers\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes = (30, 15, ), \n",
    "                          activation = \"relu\", \n",
    "                          solver = \"adam\", \n",
    "                          alpha = 0.65, \n",
    "                          max_iter = 1000, \n",
    "                          random_state = 1234)\n",
    "\n",
    "mlp_model.fit(X_train, Y_train)\n",
    "print(\"Classification rate: \", mlp_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
